---
title: "Session 3: Bayesian mixed-effects model"
author: 'Angeline Tsui'
date: '`r Sys.Date()`'
output:
  tufte::tufte_html:
    toc: yes
    toc_depth: 1
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: pdflatex
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: pdflatex
link-citations: yes

---

```{r, echo=FALSE}
library(knitr)
opts_chunk$set(echo=TRUE, 
               warning=FALSE, message=FALSE, 
               cache=FALSE,
               width = 10000)
```

This document is developed by [Angeline Tsui](mailto:angelinetsui@gmail.com), and all documents can be found [here](https://github.com/angelinetsui/srcd2021_bayesian.git).

# Introduction

This short tutorial covers some basic concepts of mixed-effect models and how to run Bayesian mixed-effect models in R. 

## Learning goals

By the end of this class, you will have a basic understanding of the following:

* **What is a mixed-effect model** 
* **The differences between fixed- and random-effects** 
* **Running Bayesian mixed-effect models using brms in R** 

# Quick overview: Regression 

## What is regression?

We often use regression to model a relationship between the independent variable $IV_i$ and dependent variable $DV$. For example, a simple form of a linear regression model can be expressed using the following equation:

$$
\text{DV} = \beta_0 + \beta_i{IV_i} + \epsilon 
$$
The $\beta_0$ is the intercept of the regression model, indicating the expected value of $DV$ when all $IV_i$ are zero. To understand the relationship between $DV$ and a particular $IV$, we can examine the regression coefficients: $\beta_i$. The $\beta_i$ indicates how $DV$ change when there is a one unit of change in $IV_i$. Thus, the $\beta_i$ indicates the strength and direction of the relationship between the $DV$ and $IV_i$.

Under the frequentist approach, we determine whether the relationship between the $DV$ and $IV_i$ is statistically significant and also use the regression model to make prediction of the $DV$ value based on specific amount of $IV_i$ values.  

Typically regression models have several assumptions, they include:

1. Linearity between the $DV$ and $IV_i$

2. Residuals are uncorrelated with one another

3. Residuals have an expected value of zero

4. Variance of residuals is constant across $IV_i$, also known as homoscedasticity

5. No multicollinearity

6. (Optional) Residuals are normally distributed

In Developmental research, we often collect data repeatedly from one individual (e.g., one child) as we are interested in the development of the children's ability over time. We cannot simply use regression models to analyze our data because we necessarily violate assumption 2: Residuals are uncorrelated with one another. In this case, we can use mixed-effect model to analyze this kind of data.

# Frequentist approach: Mixed-effect model

## What is a mixed-effect model?

Mixed-effect model is a statistical model that contains both fixed effects and random effects, and is widely used to model repeated measurements. Fixed effect are the typical $IV_i$ that we control for in a regression model, thus fixed effect are the population average effect of the $IV_i$ to the $DV$. To model the non-independence between DV within a repeated measure design, we can add random effects that capture the idiosyncratic individual differences between participants. Of course, random effects are not only bounded by the level of participants, but also can be bounded by the level of other grouping factors (e.g., laboratories). In general, random effects model the variations of the observations at the level of some grouping factors. 

Let us take a look at the [ManyBabies 1 (MB1) dataset](https://github.com/manybabies/mb1-analysis-public.git) for an illustration of how mixed-effect model can help us model repeated measurement.

In MB1, we investigated whether monolingual infants across 67 laboratories show a preference for listening to infant-directed speech (baby talk; IDS) over adult-directed speech (ADS). In this experiment, we tested infants with 16 trials where half of the trials were in IDS and the other half were in ADS. Given that repeated measure design, the mixed-effect model is a ideal method to control for the random effects of individual infants' preference for IDS over ADS. Furthermore, as we will see in the dataset, there are a lot of missing data as not all infants can complete all 16 trials. In traditional repeated ANOVA, we will either need to discard participants with missing data or use imputation method to fill out missing data. In contrast, we do not need to do that in a mixed-effect model as this model allows missing data by allocating less weight to participants with less data in the model. 

MB1 data analysis

### Step 1: Packages
```{r}
library(lme4)
library(lmerTest)
library(tidyverse)
```

### Step 2: Data import and cleaning

In this analysis, let us only focus on the laboratories that used head-turn preference procedure (HPP) with North-American English speaking babies in the analysis for a simple illustration.

```{r}
mb1_data <- read_csv("https://raw.githubusercontent.com/manybabies/mb1-analysis-public/master/processed_data/03_data_trial_main.csv") %>% 
            filter(method == "hpp", 
                   nae) %>%  #only look at HPP data and nae infants
            drop_na(.) %>% 
  mutate(age_group = factor(age_group, 
                             levels = c("3-6 mo", "6-9 mo", "9-12 mo", "12-15 mo")), #fixing the order of age group
  trial_type = factor(trial_type, 
                             levels = c("IDS", "ADS"))) #fixing the order of trial_type an d
```


### Step 3: Visualization

Let us look at the data and see infants' preference for IDS over ADS across different age and language backgrounds.

```{r}
ggplot(data = mb1_data, aes(x = trial_num, y = looking_time, color = trial_type)) +
  geom_point(alpha = 0.03, position = "jitter") +
  geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~age_group, ncol = 4, nrow = 1) +
  ylab("Looking time") +
  xlab("Trial number") + 
  scale_color_manual(values = c("red", "blue")) +
  labs(color = "Test trial type") +
  theme_classic()
```

We can notice a few things:

1. Older infants tend to look less than younger infants (at least for North American English infants).

2. All infants look less over-time, but older infants appear to have a faster rate of looking time decline than younger infants.

3. The gap between IDS and ADS is larger for older infants, suggesting that IDS preference is stronger for older infants. 


### Step 4: Analyzing the data

For simplicity, we focus on the random effects at the infant level (group) in this analysis. On the other hand, we will control for the interaction between age and IDS preference as a fixed effect. 

In the following model, our DV is a log-transformed looking time following Csibra et al (2016) suggestion as well. 

This is a much-simplified version of the model we used in MB1, just for purposes of this class.

$$
\text{log looking time} = 1 + 
\text{age * IDS} + 
(1 | \text{infant})
$$


First, let's make a model using `lme4::lmer` to show how this would look. 

```{r}
mb1_data <- mb1_data %>% 
            mutate(center_age_mo = as.numeric(scale(age_mo, scale = FALSE)),
                   log_look_time = log(looking_time), 
                   IDS = ifelse(trial_type == "IDS", 1, 0)) %>% 
  drop_na(.) 



Freq_model <- lmer(log_look_time ~ center_age_mo * IDS + 
                     (1 |subid_unique),
                   data = mb1_data) 

summary(Freq_model)$coefficients %>%
  knitr::kable(digits = 3)
```

# Bayesian approach: Mixed-effect model



### Step 1: Packages
```{r}
library(brms)
library(tidybayes)
library(bayesplot)
library(rstanarm)
```

### Step 2: Data Analysis and visualization

Before running a model with prior, let us run the model without the default prior.

```{r}
Bayes_model_default <- brm(looking_time ~ center_age_mo * IDS + 
                     (1 |subid_unique),
                     family = lognormal(),
                     data = mb1_data,
                     # extra settings to make things go faster and to save data
                     warmup = 1000, 
                     iter = 4000, 
                     cores = 2,
                     chains = 2, 
                     seed = 123,
                     save_all_pars = TRUE, 
                     control = list(adapt_delta = 0.999),
                     file="mb1_bayes.mod")


print(summary(Bayes_model_default), digits = 3)
```

Let us visualize the posterior distribution of some parameters. 
```{r}
posterior_default <- as.array(Bayes_model_default)
dim(posterior_default)
#dimnames(posterior_default) finding the names of the parameters

post_pdf_default <- mcmc_areas(posterior_default,
           pars = c("b_center_age_mo",
                    "b_IDS",
                    "b_center_age_mo:IDS"), #you can select which parameter(s) you want to plot
           prob = 0.95) + 
  ggtitle("Posterior distributions(default priors)", 
          "with medians and 95% credit intervals")

post_pdf_default

```
This plot shows the densities of four important parameter estimates. For each density, the dark blue line represents the point estimate (median) and light blue area represents the 95% credibility intervals. We can use this to get a sense of which parameters have posterior distributions that do not contain zero as well as how wide or diffuse the distribution is (e.g., is it flat or is it pointy). 

Thus far, we performed the Bayesian analysis using the default priors. But an important feature of Bayesian analysis is that we can incoporate our prior expectations into our analysis. 

### Step 3: Incoporate priors in our analyses

Let us take a look at how many parameters our model has for which we can specify priors.

```{r}
get_prior(looking_time ~ center_age_mo * IDS, data = mb1_data)
```

Note that this also tells us the we were using default priors in the previous model. For simplicity, let us set the same priors for all coefficients and variances.

Set up priors like this

```{r}
priors <- c(prior(normal(2, 1), class = Intercept), #we expect mean looking time = 2sec as this is the min looking time for the experiment         
            prior(normal(0, 0.5), class = b),
            prior(normal(0, 1), class = sigma))  
```

```{r}
Bayes_model_priors <- brm(looking_time ~ center_age_mo * IDS + 
                     (1 |subid_unique),
                   family = lognormal(),
                   prior = priors,
                   data = mb1_data,
                   warmup = 1000, 
                   iter = 4000, 
                   cores = 2,
                   chains = 2, 
                   seed = 456,
                   save_all_pars = TRUE, 
                   control = list(adapt_delta = 0.999),
                   file="mb1_bayes_priors.mod")

print(summary(Bayes_model_priors), digits = 3)
```

Let us visualize the posterior distribution of some parameters after we have incorporated some prior information.

```{r}
posterior_priors <- as.array(Bayes_model_priors)
dim(posterior_priors)
#dimnames(posterior_priors) finding the names of the parameters

prior_post_pdf <- mcmc_areas(posterior_priors,
           pars = c("b_center_age_mo",
                    "b_IDS",
                    "b_center_age_mo:IDS"), #you can select which parameter(s) you want to plot
           prob = 0.95) + 
  ggtitle("Posterior distributions", 
          "with medians and 95% credit intervals")

prior_post_pdf 

cowplot::plot_grid(post_pdf_default, prior_post_pdf, labels = c("Default", "With Prior"))

```
The posterior distributions are very similar between the models using default priors and our priors. This is because we had a fairly large sample `r `

## Step 4: Posterior predictive check

Posterior predictive check allows us to examine how well the model fits the data. It assesses the deviation between the data generated from the model and the actual data.

Let us look at the posterior predictive check for the default model:

```{r}
pp_check_default <- pp_check(Bayes_model_default)
pp_check_default
```

This posterior predictive check highlights that there is a lot of censoring in our looking time distribution, which means that our model is describing some aspects of the distribution but not others. So this is an area for future improvement of our model. 