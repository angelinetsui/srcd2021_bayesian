---
title: "Session 1: Introduction to Bayesian Inference"
author: 'Angeline Tsui & MH Tessler'
date: '`r Sys.Date()`'
output:
  tufte::tufte_html:
    toc: yes
    toc_depth: 1
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: pdflatex
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: pdflatex
link-citations: yes
---

```{r, echo=FALSE}
library(knitr)
opts_chunk$set(echo=TRUE, 
               warning=FALSE, message=FALSE, 
               cache=FALSE)
```

This document is developed by [Angeline Tsui](mailto:angelinetsui@gmail.com) and [MH Tessler](mailto:michael.h.tessler@gmail.com), and all documents can be found [here](https://github.com/angelinetsui/srcd2021_bayesian.git).

# Introduction and installation of the brms package in R

This short tutorial covers some basic concepts of Bayesian inference. 

We will use the package `brms` to illstrate some examples of running statistical modes in our workshop. `brms` is based on the probabilistic programming language Stan and you will need to install an underlying C++ compiler. Please follow the instructions [here](https://github.com/paul-buerkner/brms#faq). For Windows, you can install [Rtools](https://cran.r-project.org/bin/windows/Rtools/). For Mac, you need to install Xcode (available from the Mac App Store).

## Learning goals

By the end of this session, you will have a basic understanding of the following:

* **Probability in the Frequestist vs Probability in the Bayesian world** 
* **Bayes' Theorem** 
* **Three important elements of Bayesian statistics: prior, likelihood & posterior** 

# Probability in the Frequestist vs Probability in the Bayesian world

What is a probability? If you ask a mathematician, they will tell you "a probability is a number between 0 and 1". This may be satisfactory for those who care only about numbers, but what does a probability represent *in the world*?

Philosophers of mathematics and probability divide into two camps when it comes to answering this question: the Frequentists and the Bayesians (sometimes also called the *subjectivists*). The distinction is not only a philosophical one. In fact, the different definitions of probability give rise to different fields of statistics -- Frequentists Statistics and Bayesian Statisics -- which  has practical implications for our everyday lives as scientists. But first, let's stick at the philosophical level.

## Probability in the Frequestist world

For a Frequentist, a probability describes the *long run relative frequency of events*. For example, if I have a special coin and I say the probability that this coin lands on Heads is 0.2, what a frequentist takes that to mean is: *if I were to flip this coin many many times, then the relative frequency of the coin landing on Heads will be 20%*. 

This concept of repeating the events is fundamental to Frequentist Statistics. In the Frequestist world, probability is a long-run concept and it refers to the frequency of events when we repeat the trials (or, the generative process) to infinity. One of the most fundamental assumptions is that we can repeat the process (or the trials) in a *similar* manner for a long time. 

This assumption runs into immediate problems when a Frequentist tries to describe the probability of a singular event: For example, the probability of it raining on April 8, 2021 in Palo Alto, California is a singular event in time. How do we define what counts as a sufficiently similar event? It also has trouble when describing probabilities of probabilities. For example, what is the probability that my special coin has a probability of 0.2 of landing of Heads?

## Probability in the Bayeisan world

The Bayesian perspective does not rely on assuming we can repeat identical trials (or an identical process) a large number of times. Instead, the Bayesian sees probabilty as a measure of uncertainty about what will come to happen. That is, because we (as scientists, or as agents in the world) do not have perfect information about all of the relevant variables in the world (including interactions of subatomical particles and the like), we cannot say with certainty what will happen in the future, but we can quantify our uncertainty using probabilities. 

For example, we can estimate the probability of rain April 8, 2021 in Palo Alto, California using predictive models that encode our scientific knowledge of how whether systems evolve. These in turn might be guided by historical data (a kind of frequentist idea), but they may also include structured, theoretical knowledge about climate and weather. 

## Bayes' theorem

First of all, let us revisit the concept of conditional probability.

### Conditional probability is the probability of A given B.  

$$
P(A \mid B) = \frac{P(A \cap B)}{P(B)}
$$
For example, Let us assume for the following:

P(A) is the probability of given dormitory housing at the university = 0.2.
P(B) is the probability of getting accepted in the graduate school = 0.6.
P(A & B) = probability of given dormitory housing and being accepted in the graduate school = P(A) * P(B) = 0.2*0.6 = 0.12.
However, we know that the probability of an individual that is given a domitory housing will be higher, if the individual is accepted in the graduate school. So we want to calculate P(A | B).
P(A | B) = P(A & B)/P(B) = 0.12/0.6 = 0.2.

### Bayes' Theorem

Conditional probability forms the basics of Bayes' Theorem. Let us rearrange some equations. 

This is the conditional probability of A, given B.
$$
P(A \mid B) = \frac{P(A \cap B)}{P(B)}
$$
Thus, we can rewrite the equation and we know that the P(A & B) can be expressed in the following way:
$$
{P(A \cap B)} = P(A \mid B) * {P(B)}
$$
On the other hand, let us think of the conditional probability of B, given A
$$
P(B \mid A) = \frac{P(A \cap B)}{P(A)}
$$
We can also rewrite the equation and we know that the P(A & B) can be expressed in the following way:
$$
{P(A \cap B)} = P(B \mid A) * {P(A)}
$$
In sum, 
$$
{P(A \cap B)} = P(A \mid B) * {P(B)} = P(B \mid A) * {P(A)}
$$
This forms the Bayes' theorem where:
$$
P(B \mid A) = \frac{P(A \mid B) * {P(B)}}{P(A)}
$$
In short, Bayes' Theorem is a way of calculating “reversed” conditional probabilities. It is very useful for understanding what we want to measure in statistics within the Frequestist and Bayesian frameworks.

### Data analysis

In the Frequestist framework, we often run hypothesis testing and determine statistical significance by comparing p values with an alpha level (0.05). P value is the probability of obtaining results at least as extreme as the observed results of a statistical hypothesis test, assuming that the null hypothesis is true.

Thus, in frequestist framework, we are calculating the probability of observing some data, conditional on the probability of null hypothesis. In other words, we can rewrite this as:

$$
P value = P(data \mid h0)
$$
However, researchers are often interested in finding the probability of a hypothesis, conditional on the observed data: P(h0 | data). So what we have been doing in the frequestist framework do not necessarily provide us with what we need.

Within the Bayesian framework, we can make use of the Bayes' Theorem to find out the P(h0 | data.)
$$
P(h0 | data) = \frac{P(data | ho) * P(h0)}{P(data)}
$$
P(data | h0) is the likelihood distribition of the data, conditional on the null hypothesis
P(h0) is the prior distribution of the null hypothsis
P(data) is the marginal distribution
P(h0 | data) is the posterior distribution

## How prior and likelihood together affect the posterior?
MH: can you add your plots here? Like those in your session 19 Rmd. One concern is that I don't see the "likelihood" distribution in the Rmd (sorry that if I have missed it), so if you can add this back to the plots and make three plots (uninformative prior, weakly informative prior and very informative prior) and illustrates how these prior affect the posterior distribution, with the same likelihood distribution here, it will be great.